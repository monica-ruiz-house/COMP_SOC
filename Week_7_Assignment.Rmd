---
title: 'Week 7 Assignment: Advanced Text Analysis'
author: "Monica Ruiz House"
date: "2023-02-20"
output: word_document
---

**Thank you so much for the extension! I have started saving my files on git, so if someone happens to break my laptop again, I have all my work saved in the cloud.**

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

1.  Load in the New York Times and Breitbart text files that we used for last week's assignment. Combine the two data frames using the row bind function "rbind()" so that they are in the same data frame. Remove stopwords using the script included below. Assign each article a unique article number.

I.A - Load libraries and restructure data
```{r, message = FALSE, warning = FALSE}
# Load necessary libraries 
library(tidyverse)
library(stopwords)
library(tidytext)
library(topicmodels)


# Load our data and restructure it 
working_dr <- getwd()
breitbart <- read_csv(paste0(working_dr, "/breitbart_1000.txt"))
    breitbart$article <- rownames(breitbart)
    colnames(breitbart)<- c("text","article")
nytimes <- read_csv(paste0(working_dr, "/nytimes_1000.txt"))
    nytimes$article <- rownames(nytimes)
    colnames(nytimes)<- c("text","article")
# Distinguish between NYT & breitbart
    nytimes$article <- seq.int(nrow(nytimes)) + 1000

#input list of full texts
  bb_texts<-breitbart[ ,1]
  ny_texts<-nytimes[ ,1]
```

I.B - Removal of Stop Words
``` {r}
  letra1<-stopwords(language ="en", source = "smart")
  custom_stopwords<-append(letra1, c("mr","ms","new","york","times","breitbart","follow","twitter","facebook", "a", "p", "h", "s"))

# Remove stopwords:
#Define new "not in" function#
'%nin%' <- Negate('%in%')

#Break text into words, remove stop words, then reassemble
nytimes$text<- lapply(ny_texts, function(x) { 
                      t <- unlist(strsplit(x, " "))
                      wordlist<- t[t %nin% custom_stopwords]
                      paste(wordlist, collapse = ' ')
                       })

breitbart$text<- lapply(bb_texts, function(x) { 
                      t <- unlist(strsplit(x, " "))
                      wordlist<- t[t %nin% custom_stopwords]
                      paste(wordlist, collapse = ' ')
                       })

# Merge  both dataframes together with rbind
merged_df <- rbind(nytimes, breitbart)

```

2.  Train a topic model with 10 topics on this dataframe (it may take several minutes, so be patient). Then use the "tidy()" command from the tidytext library to convert your LDA output into two data frames. One data frame should have each row represent a document and provide "gamma" values. The other should have each row represent a word and have "beta" values.

II. A - Calculate frequencies of word and train topic model
```{r}
# Calculate the frequencies for each word and group by article
merged_wpl<- unnest_tokens(tbl = merged_df,
                        input = text,
                        output = word, 
                        token ='words') %>%
  count(article, word)

#cast_dtm(data, document, term, value,
merged_dtm <- cast_dtm(data = merged_wpl,
                    document = article,
                    term = word,
                    value = n)

merged_lda <- LDA(merged_dtm, k = 10, control = list(seed = 1234))

word_topics <- tidy(merged_lda, matrix = "beta")
doc_topics <- tidy(merged_lda, matrix = "gamma")
arrange(doc_topics, gamma)
```


3.  Use the following code block to visualize the words with the highest beta values for each topic. Then, for one topic that you find unclear, print out the words with the top 20 highest beta values. Then come up with a label for each topic. You do not need to insert this into your code, just list them here in text: (e.g. Topic 1 = "Sports", Topic 2 = "Crime")

    ```{r}
    library(ggplot2)

    top_terms <- nyt_word_topics %>%
      group_by(topic) %>%
      slice_max(beta, n = 7) %>% 
      ungroup() %>%
      arrange(topic, -beta)

    top_terms %>%
      mutate(term = reorder_within(term, beta, topic)) %>%
      ggplot(aes(beta, term, fill = factor(topic))) +
      geom_col(show.legend = FALSE) +
      facet_wrap(~ topic, scales = "free") +
      scale_y_reordered() +
      theme(strip.text.x = element_text(size = 7))
    ```

4.  Next, we want to see whether some topics are more prevalent in NYT than in Breitbart. For each periodical, make a bar chart that shows the mean Gamma value for each topic. Which topics show the greatest difference in prevalence between Breitbart and NYT? (just at a glance, no formal test necessary). +1 Extra credit if you are able to label your bar chart with the topic names you wrote for the last question (hint -- you will need to recode the values of the topic variable. Look into mutate() and recode() commands).

```{r}

```

5.  Okay, no more NYT and Breitbart! Now load in the word embeddings trained on Google Ngrams from 1900-1909 and from 1990-1999. How many rows are in each embedding? What does that mean? How many columns? What does that mean?
```{r}
1900_1909 <- read_csv(paste0(working_dr,))
1990_1999 <- read_csv(paste0(working_dr, )

```

6.  Pick a word that you think may have changed its connotations over the 20th century. Calculate the cosine similarity between that word and several other relevant words. Do you find evidence for the cultural/linguistic change that you hypothesized?
```{r}

```

7.  Next, let's identify a "gender dimension" in each embedding space. Take 4 antonym word pairs that can serve as poles for an analysis of gendered associations (e.g. "man" and "woman"). For each pair, take the difference of their word vectors, then average these together to create a "gender dimension". Now use cosine similarity to calculate projections on the gender dimension. Try to find two words that substantially change their projection (by at least 0.05) on the gender dimension between the 1900s and 1990s. (Worth 2 points).
```{r}

```
